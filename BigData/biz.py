import asyncio
import aiohttp
import json
import pandas as pd
from pyspark.sql.types import *
from utils import get_spark_session, get_hdfs_path, get_service_key

# ğŸ“Œ Spark ë° HDFS ì„¤ì •
spark = get_spark_session("BizInfoProcessing")
HDFS_PATH = get_hdfs_path()
FINAL_PATH = f"{HDFS_PATH}FinalData/Biz/FinalBiz.parquet"

# ğŸ“Œ API ì„¤ì •
SERVICE_KEY = get_service_key()

# ğŸ“Œ APIë³„ ENDPOINT
API_ENDPOINTS = {
    "BasicInfo": "http://apis.data.go.kr/1230000/ao/UsrInfoService/getPrcrmntCorpBasicInfo",
    "IndustryTypeInfo": "http://apis.data.go.kr/1230000/ao/UsrInfoService/getPrcrmntCorpIndstrytyInfo"
}

# ğŸ“Œ API ì‘ë‹µë³„ í•„í„°ë§í•  ì»¬ëŸ¼ ëª©ë¡
REQUIRED_COLUMNS = {
    "BasicInfo": "emplyeNum",
    "IndustryTypeInfo": "indstrytyCd"
}

def get_path(category, inqry_date):
    """ ì¡°íšŒ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œë“¤ì„ ë°˜í™˜ """

    return {
        "biz_info": f"{HDFS_PATH}JoinTargets/Biz/{inqry_date}/BizInfo_{category}_{inqry_date}.parquet",
        "basic_info": f"{HDFS_PATH}JoinTargets/Biz/{inqry_date}/BasicInfo_{category}_{inqry_date}.parquet",
        "industry_type": f"{HDFS_PATH}JoinTargets/Biz/{inqry_date}/IndustryType_{category}_{inqry_date}.parquet"
    }

def extract_api_targets(merged_list, biz_info_path):
    """ merged_listì™€ FinalBiz.parquetì„ ë¹„êµí•´ ê¸°ì¡´ ê¸°ì—…ì€ ë³‘í•©, ì‹ ê·œ ê¸°ì—…ì€ ë³„ë„ ì €ì¥ """

    # ê¸°ì¡´ Final Biz ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        final_data = spark.read.parquet(FINAL_PATH).toPandas()
        print(f"ğŸ“¥ ê¸°ì—… ë°ì´í„° ë¡œë”© ì™„ë£Œ ({len(final_data)}ê±´)")
    except:
        final_data = pd.DataFrame(columns=["prcbdrBizno", "prcbdrNm", "bidprcAmt", "emplyeNum", "indstrytyCd"])
        print("âš ï¸ ê¸°ì¡´ FinalBiz ì—†ìŒ â†’ ì´ˆê¸° ìƒíƒœë¡œ ì²˜ë¦¬")

    # Final Biz â†’ dict ë³€í™˜
    final_biz_dict = {
        row["prcbdrBizno"]: { # prcbdrBiznoê°€ key, prcbdrNmì™€ bidprcAmtê°€ value
            "prcbdrNm": row["prcbdrNm"], 
            "bidprcAmt": row["bidprcAmt"] if isinstance(row["bidprcAmt"], list) else [row["bidprcAmt"]],
            "emplyeNum": row.get("emplyeNum", ""),
            "indstrytyCd": row.get("indstrytyCd", "")
        }
        for _, row in final_data.iterrows() 
    }

    # ì‹ ê·œ API ìš”ì²­ ëŒ€ìƒ ì¶”ì¶œ
    api_targets = []

    # ë³‘í•© ì²˜ë¦¬
    for row in merged_list:
        bizno = row["prcbdrBizno"]
        name = row["prcbdrNm"]
        new_amts = row["bidprcAmt"]

        if bizno in final_biz_dict: # ê¸°ì¡´ Final Bizì— ì´ë¯¸ ìˆëŠ” prcbdrBiznoë¼ë©´ ìƒˆë¡œìš´ bidprcAmtë§Œ ì¶”ê°€
            existing_amts = final_biz_dict[bizno]["bidprcAmt"]
            final_biz_dict[bizno]["bidprcAmt"] = new_amts + existing_amts
        else: # ê¸°ì¡´ Final Bizì— ì—†ëŠ” prcbdrBiznoë¼ë©´ API ìš”ì²­ ëŒ€ìƒ
            api_targets.append({"prcbdrBizno": bizno, "prcbdrNm": name, "bidprcAmt": new_amts})

    # ìƒˆë¡œìš´ ê¸ˆì•¡ì„ ì¶”ê°€í•œ Final Bizë¥¼ ìƒˆë¡­ê²Œ ì €ì¥
    final_df = pd.DataFrame([
        {
            "prcbdrBizno": k,
            "prcbdrNm": v["prcbdrNm"],
            "bidprcAmt": v["bidprcAmt"],
            "emplyeNum": v["emplyeNum"],
            "indstrytyCd": v["indstrytyCd"]
        }
        for k, v in final_biz_dict.items()
    ])

    schema = StructType([
        StructField("prcbdrBizno", StringType()),
        StructField("prcbdrNm", StringType()),
        StructField("bidprcAmt", ArrayType(StringType())),
        StructField("emplyeNum", StringType()),
        StructField("indstrytyCd", StringType())
    ])

    if final_df.empty:
        print("âš ï¸ ìµœì¢… ë³‘í•© ëŒ€ìƒì´ ì—†ì–´ Spark ì €ì¥ ìƒëµ")
    else:
        spark_df = spark.createDataFrame(final_df, schema=schema)
        spark_df.write.mode("overwrite").parquet(FINAL_PATH)
        print(f"âœ… ê¸°ì¡´ ê¸°ì—… ë³‘í•© ì €ì¥ ì™„ë£Œ ({len(final_df)}ê±´)")

    # ì‹ ê·œ API ìš”ì²­ ëŒ€ìƒì€ JoinTargetì— ë”°ë¡œ ì €ì¥
    if api_targets:
        pd_api_df = pd.DataFrame(api_targets)
        spark_api_df = spark.createDataFrame(pd_api_df)
        spark_api_df.write.mode("overwrite").parquet(biz_info_path)
        print(f"ğŸ“¦ ì‹ ê·œ API ëŒ€ìƒ ì €ì¥ ì™„ë£Œ ({len(api_targets)}ê±´)")
    else:
        print("âœ… ì‹ ê·œ API ìš”ì²­ ëŒ€ìƒ ì—†ìŒ")

def merge_biz_list(new_biz_list: list[dict]) -> list[dict]:
    """ new_biz_listë¥¼ prcbdrBiznoë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ list[dict] ë°˜í™˜"""
    biz_dict = {}

    for biz in new_biz_list:
        bizno = biz["prcbdrBizno"]
        name = biz["prcbdrNm"]
        amount = biz["bidprcAmt"]

        if bizno in biz_dict: # ì¤‘ë³µëœ ì‚¬ì—…ì ë²ˆí˜¸ë¼ë©´
            biz_dict[bizno]["bidprcAmt"].insert(0, amount) # ê¸°ì¡´ì˜ í–‰ì— ê¸ˆì•¡ë§Œ ì¶”ê°€
        else:
            biz_dict[bizno] = {
                "prcbdrNm": name,
                "bidprcAmt": [amount]
            }

    return [
        {"prcbdrBizno": k, "prcbdrNm": v["prcbdrNm"], "bidprcAmt": v["bidprcAmt"]}
        for k, v in biz_dict.items()
    ]

# ê¸°ì—… ë°ì´í„° êµ¬ì¶•ì„ ìœ„í•œ ì¶”ê°€ì ì¸ API ìš”ì²­ ë° ì‘ë‹µ ë³‘í•©

def load_biz_info(biz_info_path):
    print(f"ğŸ“¥ HDFSì—ì„œ ê¸°ì—… ì •ë³´ ë°ì´í„° ë¡œë”© ì¤‘")

    df = spark.read.parquet(biz_info_path)
    biz_list = [row.prcbdrBizno for row in df.select("prcbdrBizno").distinct().collect()]
    print(f"âœ… ì´ {len(biz_list)}ê±´ì˜ ì‚¬ì—…ì ë²ˆí˜¸ ë¡œë“œ ì™„ë£Œ")
    return biz_list

async def fetch_BasicInfo(session, biz_no, api_name):
    """[ì‚¬ìš©ìì •ë³´ì„œë¹„ìŠ¤] ì¡°ë‹¬ì—…ì²´ ê¸°ë³¸ì •ë³´"""
    params = {
        "serviceKey":SERVICE_KEY,
        "pageNo":1,
        "numOfRows":1,
        "inqryDiv":3, # ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸
        "bizno":biz_no,
        "type":"json"
    }
    return await fetch_api_data(session, api_name, biz_no, params)

async def fetch_IndustryTypeInfo(session, biz_no, api_name):
    """[ì‚¬ìš©ìì •ë³´ì„œë¹„ìŠ¤] ì¡°ë‹¬ì—…ì²´ ì—…ì¢…ì •ë³´ ì¡°íšŒ"""
    params = {
        "serviceKey":SERVICE_KEY,
        "pageNo":1,
        "numOfRows":1,
        "inqryDiv":1, # ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸
        "bizno":biz_no,
        "type":"json"
    }
    return await fetch_api_data(session, api_name, biz_no, params)

MAX_RETRIES = 3
RETRY_DELAY = 1

# 3ï¸âƒ£ API ìš”ì²­
async def fetch_api_data(session, api_name, biz_no, params):
    headers = {
        "Accept": "application/json",
        "User-Agent": "Mozilla/5.0 (compatible; Python aiohttp client)"
    }

    for attempt in range(MAX_RETRIES):
        try:
            async with session.get(API_ENDPOINTS.get(api_name), params=params, headers=headers, timeout=aiohttp.ClientTimeout(total=8)) as response:
                content_type = response.headers.get("Content-Type", "")
                text = await response.text()

                # âœ… JSON ì‘ë‹µì¸ ê²½ìš°
                if "application/json" in content_type:
                    json_data = json.loads(text)
                    items = json_data.get("response", {}).get("body", {}).get("items", [])
                    if items:
                        filtered = {REQUIRED_COLUMNS.get(api_name): items[0].get(REQUIRED_COLUMNS.get(api_name), "")}
                        filtered["prcbdrBizno"] = biz_no
                        return filtered

                # â— XML ì‘ë‹µì´ì§€ë§Œ ìš”ì²­ ì œí•œ ì—ëŸ¬ì¼ ê²½ìš°
                if "LIMITED_NUMBER_OF_SERVICE_REQUESTS_PER_SECOND_EXCEEDS_ERROR" in text:
                    #print(f"â³ {bid_info[0]} ìš”ì²­ ì œí•œ ì´ˆê³¼, {RETRY_DELAY}ì´ˆ í›„ ì¬ì‹œë„... (ì‹œë„ {attempt + 1}/{MAX_RETRIES})")
                    await asyncio.sleep(RETRY_DELAY)
                    continue

                # â— ê¸°íƒ€ XML ì‘ë‹µ
                #print(f"âš ï¸ {biz_no} ì‘ë‹µì´ JSONì´ ì•„ë‹˜: {content_type}")
                print(biz_no, " ", api_name, "\n", text[:300])  # ë””ë²„ê¹…ìš© ì¼ë¶€ ì¶œë ¥
                break

        except Exception as e:
            #print(f"âš ï¸ {biz_no} ìš”ì²­ ì˜ˆì™¸ (ì‹œë„ {attempt + 1}/{MAX_RETRIES}): {e}")
            await asyncio.sleep(RETRY_DELAY)

    # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    return {"prcbdrBizno": biz_no}

# 1ï¸âƒ£ api_nameì— í•´ë‹¹í•˜ëŠ” í•¨ìˆ˜ ë§¤ì¹­ & 2ï¸âƒ£ ìš”ì²­ ë³‘ë ¬ ì‹¤í–‰
async def fetch_response(biz_list, api_name, max_connections=10):
    fetch_function = API_FETCH_FUNCTIONS.get(api_name) # api_nameì— í•´ë‹¹í•˜ëŠ” í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°

    connector = aiohttp.TCPConnector(limit=max_connections)
    async with aiohttp.ClientSession(connector=connector) as session:
        # 1ï¸âƒ£ api_nameì— í•´ë‹¹í•˜ëŠ” í•¨ìˆ˜ ë§¤ì¹­
        tasks = [fetch_function(session, biz_no, api_name) for biz_no in biz_list]
        results = []

        # 2ï¸âƒ£ ìš”ì²­ ë³‘ë ¬ ì‹¤í–‰
        for i in range(0, len(tasks), 100):  # 100ê°œì”© ì‹¤í–‰í•˜ë©° ì§„í–‰ë¥  ì¶”ì 
            batch = tasks[i:i+100]
            batch_result = await asyncio.gather(*batch)
            await asyncio.sleep(0.2)
            results.extend(batch_result)
            print(f"ğŸ”„ ì§„í–‰ë¥ : {min(i+100, len(tasks))}/{len(tasks)}ê±´ ì™„ë£Œ")
        return results

# 0ï¸âƒ£  ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜
def save_response_to_parquet(biz_list, api_name, output_path):
    print(f"ğŸš€ {api_name} ìš”ì²­ ì‹œì‘...")

    # 1ï¸âƒ£ api_nameì— í•´ë‹¹í•˜ëŠ” í•¨ìˆ˜ ë§¤ì¹­ & 2ï¸âƒ£ ìš”ì²­ ë³‘ë ¬ ì‹¤í–‰
    all_data = asyncio.run(fetch_response(biz_list, api_name))

    # 4ï¸âƒ£ ì‘ë‹µì„ parquetë¡œ HDFSì— ì €ì¥
    print("ğŸ“¦ Pandas DataFrame ìƒì„± ì¤‘...")
    pd_df = pd.DataFrame(all_data)

    spark_schema = StructType([StructField(col, StringType(), True) for col in pd_df.columns])
    spark_df = spark.createDataFrame(pd_df, schema=spark_schema)

    print(f"ğŸ’¾ Parquet ì €ì¥ ì¤‘: {output_path}")
    spark_df.write.mode("overwrite").parquet(output_path)

def join_data(path):
    """ ì‚¬ì—…ì ë“±ë¡ ë²ˆí˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ parquet íŒŒì¼ì„ JOIN """

    print("ğŸ“¥ Parquet íŒŒì¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    df1 = spark.read.parquet(path.get("biz_info"))
    df2 = spark.read.parquet(path.get("basic_info"))
    df3 = spark.read.parquet(path.get("industry_type"))

    print("ğŸ”— DataFrame ë³‘í•© ì¤‘ (ê¸°ì¤€: prcbdrBizno)...")
    joined_df = (
        df1.join(df2, "prcbdrBizno", "outer") 
           .join(df3, "prcbdrBizno", "outer")
    )

    print("ğŸ” ìƒ˜í”Œ ì¶œë ¥:")
    joined_df.show(10)

    print("ğŸ“ ê¸°ì¡´ FinalBiz ë¶ˆëŸ¬ì˜¤ê¸°...")
    try:
        existing_df = spark.read.parquet(FINAL_PATH).cache()
        existing_df.count()
        print(f"âœ… ê¸°ì¡´ ë°ì´í„° ìˆ˜: {existing_df.count()}")
    except:
        print("âš ï¸ ê¸°ì¡´ FinalBiz ì—†ìŒ â†’ ìƒˆë¡œ ìƒì„± ì‹œì‘")
        existing_df = spark.createDataFrame([], joined_df.schema)

    print("â• ì‹ ê·œ ë°ì´í„° append ì¤‘...")
    final_df = existing_df.unionByName(joined_df)

    print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥: {FINAL_PATH}")
    final_df \
        .coalesce(1) \
        .write.mode("overwrite").parquet(FINAL_PATH)

    print("âœ… ìµœì¢… FinalBiz ì €ì¥ ì™„ë£Œ! ì´ ë°ì´í„° ìˆ˜:", final_df.count())

def collect_biz_data(path):
    """ ì¶”ê°€ì ì¸ API ìš”ì²­ í›„ ë°ì´í„° ë³‘í•© """

    biz_list = load_biz_info(path.get("biz_info")) # ğŸ“Œ ìƒˆë¡œ ìˆ˜ì§‘í•´ì•¼í•˜ëŠ” ì‚¬ì—…ì ë“±ë¡ ë²ˆí˜¸ ì¡°íšŒ

    save_response_to_parquet(biz_list, "BasicInfo", path.get("basic_info"))
    save_response_to_parquet(biz_list, "IndustryTypeInfo", path.get("industry_type"))

    join_data(path)

# ğŸ“Œ API ìš”ì²­ë³„ í•¨ìˆ˜ ë§¤í•‘
API_FETCH_FUNCTIONS = {
    "BasicInfo": fetch_BasicInfo,
    "IndustryTypeInfo": fetch_IndustryTypeInfo 
}

def merge_and_save_bizinfo(category, inqry_date, new_biz_list):
    """ collect.pyì—ì„œ ìƒˆë¡­ê²Œ ë°›ì€ new_biz_listë¥¼ ì¼ë‹¨ ì‚¬ì—…ì ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© """

    path = get_path(category, inqry_date)

    # ì‹ ê·œë¡œ ë°›ì€ new_biz_listë¥¼ ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    merge_list = merge_biz_list(new_biz_list)

    # ë³‘í•©ëœ ì‹ ê·œ merge_list ì¤‘ ìƒˆë¡œ API ìš”ì²­ì„ ë³´ë‚´ì•¼í•˜ëŠ” ëŒ€ìƒë§Œ ì¶”ì¶œ
    extract_api_targets(merge_list, path.get("biz_info"))

    # API ìš”ì²­
    collect_biz_data(path)

if __name__ == "__main__":
    path = get_path("ConstructionWork", "202412")
    join_data(path)
