# ~/Project/daily/data_loader.py
"""
HDFSì—ì„œ ë°ì´í„° ë¡œë“œ ëª¨ë“ˆ
"""
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, size
from hdfs import InsecureClient
import config_daily as config
from urllib.parse import urlparse

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


def create_spark_session(app_name="Bid Data Loader"):
    """Spark ì„¸ì…˜ ìƒì„±"""
    spark = SparkSession.builder \
        .appName(app_name) \
        .master("local[4]") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:8020") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")
    return spark


def get_hdfs_client():
    """WebHDFS í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return InsecureClient("http://namenode:9870")  # í¬íŠ¸ëŠ” í™˜ê²½ì— ë§ê²Œ ì¡°ì •


# def check_hdfs_path(path):
#     """WebHDFSë¥¼ ì´ìš©í•´ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
#     logger.info(f"ğŸ” WebHDFSë¡œ ê²½ë¡œ í™•ì¸ ì¤‘: {path}")
#     try:
#         client = get_hdfs_client()
#         exists = client.status(path, strict=False) is not None
#         logger.info(f"  - ê²°ê³¼: {'ìˆìŒ' if exists else 'ì—†ìŒ'}")
#         return exists
#     except Exception as e:
#         logger.error(f"âŒ WebHDFS ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {e}")
#         return False


def check_hdfs_path(path):
    logger.info(f"ğŸ” WebHDFSë¡œ ê²½ë¡œ í™•ì¸ ì¤‘: {path}")
    try:
        # hdfs://namenode:8020/user/hadoop/data/bids/20250407 â†’ /user/hadoop/data/bids/20250407
        parsed_path = urlparse(path).path
        client = get_hdfs_client()
        exists = client.status(parsed_path, strict=False) is not None
        logger.info(f"  - ê²°ê³¼: {'ìˆìŒ' if exists else 'ì—†ìŒ'}")
        return exists
    except Exception as e:
        logger.error(f"âŒ WebHDFS ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False



def load_todays_data(spark):
    """HDFSì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œ ë°ì´í„° ë¡œë“œ"""
    today_path = config.get_today_data_path()
    logger.info(f"ğŸ”„ ì˜¤ëŠ˜ ë°ì´í„° ë¡œë”© ì¤‘: {today_path}")

    if not check_hdfs_path(today_path):
        logger.error(f"âŒ HDFSì— ì˜¤ëŠ˜ ë‚ ì§œ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {today_path}")
        return None

    try:
        # Parquet íŒŒì¼ ë¡œë“œ
        df = spark.read.parquet(today_path)
        count = df.count()
        logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {count}ê°œ ë ˆì½”ë“œ")

        # ìœ íš¨ì„± ê²€ì‚¬
        non_empty_bizs = df.filter(size(col("bizs_parsed")) > 0).count()
        logger.info(f"ğŸ“Š ìœ íš¨í•œ bizs_parsed: {non_empty_bizs}/{count} ({non_empty_bizs / count * 100:.2f}%)")

        # ìƒ˜í”Œ recentprc í™•ì¸
        recentprc_sample = df.select(
            "bidNtceNo",
            col("bizs_parsed")[0]["prcbdrBizno"].alias("prcbdrBizno"),
            col("bizs_parsed")[0]["recentprc"].alias("recentprc")
        ).limit(3)

        logger.info("ğŸ” recentprc ìƒ˜í”Œ (bidprcAmtì˜ ì²« ë²ˆì§¸ ê°’):")
        recentprc_sample.show(truncate=False)

        return df

    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise
