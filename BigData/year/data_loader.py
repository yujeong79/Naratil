"""
HDFSì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ëª¨ë“ˆ
"""
import logging
import pandas as pd
from pyspark.sql import SparkSession
from . import config_year as config  # â† ìˆ˜ì •ëœ ë¶€ë¶„

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

def create_spark_session():
    """Spark ì„¸ì…˜ ìƒì„± - ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”"""
    spark = SparkSession.builder \
        .appName(config.SPARK_APP_NAME) \
        .master(config.SPARK_MASTER) \
        .config("spark.driver.memory", config.SPARK_DRIVER_MEMORY) \
        .config("spark.executor.memory", config.SPARK_EXECUTOR_MEMORY) \
        .config("spark.sql.shuffle.partitions", config.SPARK_SHUFFLE_PARTITIONS) \
        .config("spark.network.timeout", config.SPARK_NETWORK_TIMEOUT) \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    logger.info("âœ… Spark ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
    return spark

def load_stopwords(path):
    """ë¶ˆìš©ì–´ ì‚¬ì „ ë¡œë“œ"""
    try:
        stopwords = set(pd.read_csv(path)["stopword"].tolist())
        logger.info(f"âœ… ë¶ˆìš©ì–´ {len(stopwords)}ê°œ ë¡œë“œ ì™„ë£Œ")
        return stopwords
    except Exception as e:
        logger.error(f"âŒ ë¶ˆìš©ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return set()

def load_data_from_hdfs(spark):
    """HDFSì—ì„œ parquet ë°ì´í„° ë¡œë“œ"""
    try:
        logger.info(f"ğŸ“¦ HDFSì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘: {config.HDFS_PATH}")
        df = spark.read.parquet(config.HDFS_PATH)
        logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.count()} í–‰")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        logger.info("ğŸ” ë°ì´í„° ìƒ˜í”Œ:")
        df.select("bidNtceNo", "bidNtceNm", "industry_code").show(2, truncate=False)
        
        return df
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
