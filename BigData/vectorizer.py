# ~/Project/daily/vectorizer.py
"""
í…ìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™” ì²˜ë¦¬ ëª¨ë“ˆ
"""
import logging
import pandas as pd
import re
import os
from pyspark.sql.functions import col, when, pandas_udf
from pyspark.sql.types import ArrayType, FloatType
from sentence_transformers import SentenceTransformer

import config_daily as config

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ëª¨ë¸ ìºì‹± í•¨ìˆ˜
def get_model():
    """ëª¨ë¸ ë¡œë“œ (ìºì‹± ì ìš©)"""
    if not hasattr(get_model, "model"):
        logger.info("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘ (ìºì‹±ëœ ëª¨ë¸ ì‚¬ìš©)...")
        get_model.model = SentenceTransformer(config.VECTOR_MODEL)
        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ìºì‹±ë¨)")
    return get_model.model

# ë¶ˆìš©ì–´ ë¡œë“œ í•¨ìˆ˜ (ìºì‹± ì ìš©)
def load_stopwords():
    """ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (ìºì‹± ì ìš©)"""
    if not hasattr(load_stopwords, "stopwords"):
        stopwords_path = config.STOPWORDS_PATH
        try:
            if os.path.exists(stopwords_path):
                df = pd.read_csv(stopwords_path, header=None)
                load_stopwords.stopwords = set(df[0].str.strip())
                logger.info(f"ğŸ“š ë¶ˆìš©ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ: {len(load_stopwords.stopwords)}ê°œ")
            else:
                logger.warning(f"âš ï¸ ë¶ˆìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stopwords_path}")
                load_stopwords.stopwords = set()
        except Exception as e:
            logger.error(f"âŒ ë¶ˆìš©ì–´ ë¡œë“œ ì˜¤ë¥˜: {e}")
            load_stopwords.stopwords = set()
    return load_stopwords.stopwords

# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (ë¶ˆìš©ì–´ ì œê±° í¬í•¨)
def clean_and_filter(texts: pd.Series, stopwords: set) -> pd.Series:
    """í…ìŠ¤íŠ¸ ì •ì œ ë° ë¶ˆìš©ì–´ ì œê±°"""
    # íŠ¹ìˆ˜ë¬¸ì, ê´„í˜¸ ë‚´ìš© ì œê±°
    cleaned = texts.fillna("").apply(lambda x: re.sub(r"\[.*?\]|\(.*?\)|\{.*?\}|<.*?>", "", x))
    cleaned = cleaned.apply(lambda x: re.sub(r"[#@!*%^&]", "", x))
    cleaned = cleaned.apply(lambda x: re.sub(r"\s+", " ", x).strip())
    
    # ì—¬ê¸°ì„œ ë¶ˆìš©ì–´ ì œê±°
    #cleaned = cleaned.apply(lambda x: " ".join([w for w in x.split() if w not in stopwords]))
    cleaned = cleaned.apply(
    lambda x: re.sub(r"\s+", " ", 
                     "".join([x.replace(word, "") for word in stopwords])
                    ).strip()
)

    return cleaned

def vectorize_data(spark, df):
    """ë°ì´í„° ë²¡í„°í™”"""
    logger.info("âš™ï¸ ë°ì´í„° ë²¡í„°í™” ì‹œì‘...")
    
    # ë¶ˆìš©ì–´ ë¡œë“œ
    stopwords = load_stopwords()
    
    # UDF ì •ì˜
    @pandas_udf(ArrayType(FloatType()))
    def vectorize_udf(texts: pd.Series) -> pd.Series:
        # ë¶ˆìš©ì–´ ì œê±°ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ ì •ì œ
        cleaned = clean_and_filter(texts, stopwords)
        model = get_model()
        
        # ì²­í¬(ë°°ì¹˜) ë‹¨ìœ„ë¡œ ë²¡í„°í™” ì²˜ë¦¬
        all_vectors = []
        for i in range(0, len(cleaned), config.BATCH_SIZE):
            batch_texts = cleaned.iloc[i:i + config.BATCH_SIZE].tolist()
            vectors = model.encode(batch_texts, convert_to_numpy=True)
            all_vectors.extend(vectors.tolist())
        
        return pd.Series(all_vectors)
    
    # ë²¡í„°í™” ì ìš©
    df_vector = df.withColumn("vectorNm", vectorize_udf(col("bidNtceNm")))
    
    # ë°ì´í„° null ì²˜ë¦¬
    columns_to_clean = [c for c in df_vector.columns if c != "vectorNm" and c != "bizs_parsed"]
    for col_name in columns_to_clean:
        df_vector = df_vector.withColumn(
            col_name,
            when((col(col_name) == "") | (col(col_name) == "NaN"), None).otherwise(col(col_name))
        )
    
    logger.info("âœ… ë²¡í„°í™” ë° null ì •ì œ ì™„ë£Œ")
    
    # ë²¡í„°í™” ê²°ê³¼ ìƒ˜í”Œ í™•ì¸
    sample_vectors = df_vector.select("bidNtceNm", "vectorNm").limit(2)
    logger.info("ğŸ” ë²¡í„°í™” ê²°ê³¼ ìƒ˜í”Œ:")
    for row in sample_vectors.collect():
        logger.info(f"  ì›ë³¸ í…ìŠ¤íŠ¸: {row['bidNtceNm']}")
        logger.info(f"  ë²¡í„° ì°¨ì›: {len(row['vectorNm'])}")
        logger.info(f"  ë²¡í„° ì¼ë¶€: {row['vectorNm'][:5]}...")  # ì•ë¶€ë¶„ 5ê°œ ìš”ì†Œë§Œ ì¶œë ¥
        logger.info("---")
    
    return df_vector
