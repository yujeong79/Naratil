import re
import pandas as pd
import numpy as np
import time
import os

from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf, col, when, avg, count, lit
from pyspark.sql.types import ArrayType, FloatType, StringType
from sentence_transformers import SentenceTransformer

# ==============================
# âœ… ìš”êµ¬ì‚¬í•­ ì •ë¦¬
# ==============================
# 1. lcnsLmtNmì„ ì—…ì¢…ëª… / ì½”ë“œë¡œ ë¶„ë¦¬í•˜ì—¬ ê°ê° lcnsLmtNm, Industry_code ì»¬ëŸ¼ì— ì €ì¥
# 2. ë²¡í„°í™”ëŠ” bidNtceNm ê¸°ì¤€ìœ¼ë¡œ 768ì°¨ì› ë²¡í„° ìƒì„±
# 3. null, ë¹ˆ ë¬¸ìì—´, "NaN", np.nan â†’ ëª¨ë‘ null ì²˜ë¦¬
# 4. nullì´ì–´ë„ jsonì— ëˆ„ë½ë˜ì§€ ì•Šê³  "ì»¬ëŸ¼ëª…": null í˜•íƒœë¡œ ì¶œë ¥ë˜ê²Œ í•˜ê¸°
# 5. Industry_codeë³„ ê³µê³  ìˆ˜ ì§‘ê³„ í›„
#    - null â†’ no_limit í´ë”ì— ì €ì¥
#    - í‰ê·  ë¯¸ë§Œ â†’ etc í´ë”ì— ì €ì¥
#    - í‰ê·  ì´ìƒ â†’ Industry_codeë³„ ë‹¨ì¼ í´ë” ì €ì¥
# 6. ëª¨ë¸ì€ ìºì‹±í•˜ì—¬ ì—¬ëŸ¬ ë²ˆ ë¡œë”© ë°©ì§€
# ==============================

# ê²½ë¡œ ì„¤ì •
PARQUET_PATH = "hdfs://localhost:9000/user/hadoop/data/FinalData/Thing/Final_Thing_202410.parquet"
STOPWORDS_PATH = "data/stopwords.csv"
OUTPUT_DIR = "data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

start_time = time.time()
print("\nğŸš€ Spark ì‹œì‘...")

# Spark ì„¸ì…˜ ìƒì„±
spark = SparkSession.builder \
    .appName("Vectorize and Split") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# ë°ì´í„° ë¡œë”©
print("\nğŸ“¦ Parquet ë°ì´í„° ë¡œë”© ì¤‘...")
df = spark.read.parquet(PARQUET_PATH).repartition(8)
print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.count()} rows")

# Stopword ë¶ˆëŸ¬ì˜¤ê¸°
stopwords = set(pd.read_csv(STOPWORDS_PATH)["stopword"].tolist())

# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
def clean_and_filter(texts: pd.Series) -> pd.Series:
    cleaned = texts.fillna("").apply(lambda x: re.sub(r"\[.*?\]|\(.*?\)|\{.*?\}|<.*?>", "", x))
    cleaned = cleaned.apply(lambda x: re.sub(r"[#@!*%^&]", "", x))
    cleaned = cleaned.apply(lambda x: re.sub(r"\s+", " ", x).strip())
    cleaned = cleaned.apply(lambda x: " ".join([w for w in x.split() if w not in stopwords]))
    return cleaned

# ëª¨ë¸ ìºì‹± í•¨ìˆ˜
def get_model():
    if not hasattr(get_model, "model"):
        print("\nğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘ (ìºì‹±ëœ ëª¨ë¸ ì‚¬ìš©)...")
        get_model.model = SentenceTransformer("jhgan/ko-sbert-sts")
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ìºì‹±ë¨)")
    return get_model.model

# UDF ì •ì˜
@pandas_udf(ArrayType(FloatType()))
def vectorize_udf(texts: pd.Series) -> pd.Series:
    cleaned = clean_and_filter(texts)
    model = get_model()
    vectors = model.encode(cleaned.tolist(), convert_to_numpy=True)
    return pd.Series([vec.tolist() for vec in vectors])

@pandas_udf(StringType())
def extract_name_udf(col_text: pd.Series) -> pd.Series:
    return col_text.apply(lambda x: x.split("/")[0].strip() if isinstance(x, str) and "/" in x else None)

@pandas_udf(StringType())
def extract_code_udf(col_text: pd.Series) -> pd.Series:
    return col_text.apply(lambda x: x.split("/")[-1].strip() if isinstance(x, str) and "/" in x else None)

# UDF ì ìš©
print("\nâš™ï¸ ë²¡í„°í™” ë° ì—…ì¢… íŒŒì‹± ì¤‘...")
df_vector = (
    df.withColumn("Industry_code", extract_code_udf(col("lcnsLmtNm")))
      .withColumn("lcnsLmtNm", extract_name_udf(col("lcnsLmtNm")))
      .withColumn("vectorNm", vectorize_udf(col("bidNtceNm")))
)

# ëª¨ë“  ì»¬ëŸ¼ì—ì„œ ê³µë°±/NaN/null í†µí•© ì •ì œ
columns_to_clean = [c for c in df_vector.columns if c != "vectorNm"]
for col_name in columns_to_clean:
    df_vector = df_vector.withColumn(
        col_name,
        when((col(col_name) == "") | (col(col_name) == "NaN"), None).otherwise(col(col_name))
    )

print("âœ… null ì •ì œ ì™„ë£Œ")

# ê³µê³  ìˆ˜ ì§‘ê³„ ë° í‰ê·  ê³„ì‚°
industry_count = df_vector.groupBy("Industry_code").agg(count("bidNtceNo").alias("cnt"))
avg_count = industry_count.agg(avg("cnt")).first()[0]
print(f"ğŸ“Š Industry í‰ê·  ê³µê³  ìˆ˜: {avg_count:.2f}")

# ì¡°ê±´ í•„í„°ë§
df_vector = df_vector.join(industry_count, on="Industry_code", how="left")
no_limit_df = df_vector.filter(col("Industry_code").isNull())
etc_df = df_vector.filter((col("cnt") < avg_count) & col("Industry_code").isNotNull())
greater_df = df_vector.filter(col("cnt") >= avg_count)

# ì €ì¥
print("\nğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
no_limit_df.coalesce(1).write.mode("overwrite").json(f"{OUTPUT_DIR}/no_limit")
etc_df.coalesce(1).write.mode("overwrite").json(f"{OUTPUT_DIR}/etc")

for row in greater_df.select("Industry_code").distinct().collect():
    code = row["Industry_code"]
    print(f"âœ… {code} ì—…ì¢… ì €ì¥ ì¤‘...")
    df_code = greater_df.filter(col("Industry_code") == code)
    df_code.coalesce(1).write.mode("overwrite").json(f"{OUTPUT_DIR}/{code}")

print("âœ… ëª¨ë“  JSON ì €ì¥ ì™„ë£Œ")
print("â±ï¸ ì´ ì†Œìš” ì‹œê°„: {:.2f}ì´ˆ".format(time.time() - start_time))
