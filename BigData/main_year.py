"""
HDFS ë°ì´í„° ë²¡í„°í™” ë° MongoDB ì „ì†¡ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸


import time
import logging
import traceback
import os
from pyspark.sql.functions import col, avg

# ì„¤ì • ë° ëª¨ë“ˆ import
import config
from modules import (
    create_spark_session, 
    load_data_from_hdfs, 
    load_stopwords,
    vectorize_data,
    split_by_industry_code,
    get_industry_dfs,
    send_to_mongodb,
    init_mongodb,
    handle_company_full
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=config.LOG_LEVEL,
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)
"""
import time
import logging
import traceback
import os
from pyspark.sql.functions import col, avg

# ì„¤ì • ë° ëª¨ë“ˆ import
import year.config_year as config  # â† ìˆ˜ì •ëœ ë¶€ë¶„
from year import (
    create_spark_session, 
    load_data_from_hdfs, 
    load_stopwords,
    vectorize_data,
    split_by_industry_code,
    get_industry_dfs,
    send_to_mongodb,
    init_mongodb,
    handle_company_full
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=config.LOG_LEVEL,
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = time.time()
    logger.info("ğŸš€ HDFS ë°ì´í„° ë²¡í„°í™” ë° MongoDB ì „ì†¡ ì‹œì‘...")
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.TEMP_DIR, exist_ok=True)
    
    try:
        # 1. MongoDB ì—°ê²° ì´ˆê¸°í™”
        if not init_mongodb():
            logger.error("MongoDB ì—°ê²° ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        # 2. Spark ì„¸ì…˜ ìƒì„±
        spark = create_spark_session()
        
        # 3. ë¶ˆìš©ì–´ ë¡œë“œ
        stopwords = load_stopwords(config.STOPWORDS_PATH)
        
        # 4. HDFSì—ì„œ ë°ì´í„° ë¡œë“œ
        df = load_data_from_hdfs(spark)
        
        # 5. ë°ì´í„° ë²¡í„°í™”
        df_vector = vectorize_data(spark, df, stopwords)
        
        # 6. ì—…ì¢… ì½”ë“œë³„ ë°ì´í„° ë¶„ë¦¬
        no_limit_df, etc_df, greater_df, industry_count = split_by_industry_code(df_vector)
        
        # 7. MongoDBë¡œ ë°ì´í„° ì „ì†¡
        
        # 7.0 company
        #handle_company_full(spark)
        
        #######
        # 7.3 ì—…ì¢…ë³„ ê°œë³„ ì»¬ë ‰ì…˜ ì „ì†¡ (4989ë§Œ ë¨¼ì € ì²˜ë¦¬)
        greater_success = 0
        avg_value = industry_count.agg(avg("cnt")).first()[0]

        industry_dfs = get_industry_dfs(greater_df, industry_count, avg_value)
        logger.info(f"ğŸ“¦ ì—…ì¢… ì½”ë“œ each ì²˜ë¦¬")

        # 4989 ì—…ì¢…ë§Œ ë¨¼ì € ì°¾ì•„ì„œ ì²˜ë¦¬
        found_4989 = False
        target_code = 4989
        for code, df_code, count in industry_dfs:
             if str(code) == str(target_code):
                logger.info(f"ğŸ“¦ ì—…ì¢… ì½”ë“œ {code} ë¨¼ì € ì²˜ë¦¬ ì¤‘ (ì´ {count}ê±´)...")
                logger.info(f"  (ì—…ì¢… ì½”ë“œ íƒ€ì…: {type(code)})")
                
                sample = df_code.select("bidNtceId", "bidNtceNo").limit(3).collect()
                logger.info(f"industry_{code} bidNtceId ìƒ˜í”Œ:")
                for row in sample:
                    logger.info(f"  bidNtceId: {row['bidNtceId']}, bidNtceNo: {row['bidNtceNo']}")
                # MongoDBë¡œ ì „ì†¡
                code_success = send_to_mongodb(df_code, f"industry_{code}")
                greater_success += code_success
                found_4989 = True
                break  # 4989 ì²˜ë¦¬ í›„ ë£¨í”„ ì¢…ë£Œ

        # 4989ë¥¼ ì°¾ì§€ ëª»í–ˆì„ ê²½ìš° ë¡œê·¸ ì¶œë ¥
        if not found_4989:
            logger.warning("âš ï¸ ì—…ì¢… ì½”ë“œ 4989ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 7.2 industry_etc ì»¬ë ‰ì…˜ ì „ì†¡
        etc_success = send_to_mongodb(etc_df, "industry_etc")

# ë‚˜ë¨¸ì§€ ì—…ì¢… ì½”ë“œëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        
        
        # 7.1 no_limit ì»¬ë ‰ì…˜ ì „ì†¡
        #no_limit_success = send_to_mongodb(no_limit_df, "industry_no_limit")
        total_success = etc_success + greater_success
        total_count = etc_df.count() + greater_df.count()
        
        logger.info("ğŸ‰ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        logger.info(f"  - ì´ ì²˜ë¦¬ ê±´ìˆ˜: {total_count}ê±´")
        logger.info(f"  - ì „ì†¡ ì„±ê³µ ê±´ìˆ˜: {total_success}ê±´ ({total_success/total_count*100:.2f}%)")
        logger.info(f"  - ì´ ì†Œìš” ì‹œê°„: {(time.time() - start_time):.2f}ì´ˆ")
        
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
    finally:
        # Spark ì„¸ì…˜ ì¢…ë£Œ
        if 'spark' in locals():
            spark.stop()
            logger.info("Spark ì„¸ì…˜ ì¢…ë£Œ")

if __name__ == "__main__":
    main()