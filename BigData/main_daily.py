# ìì •ì— ì‹¤í–‰.
"""
ë©”ì¸ ì‹¤í–‰ íŒŒì¼

processAll: csv ì›ë³¸ -> ì •ì œ(parsing, vectorization, json) -> MongoDB í´ë¼ìš°ë“œ Upload (atlas)
1. processAll ì•ˆì—ì„œ CSV -> HDFS ì ì¬ (bidprcAmtì˜ ì²« ë²ˆì§¸ ê°’ì„ recentprcë¡œ ì €ì¥)
2. HDFS -> ë²¡í„°í™” -> ë°ì´í„° ë¶„ë¥˜ -> MongoDB ì €ì¥
"""
import logging
import time
from datetime import datetime

import config_daily as config
from data_loader import load_todays_data, create_spark_session
from vectorizer import vectorize_data
from data_processor import process_data
from mongodb_sender import save_to_mongodb
from csvTohdfs import process_csv_files

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def process_all():
    """
    ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰:
    1. CSV -> HDFS ì ì¬ (company_full ë°ì´í„° ì²˜ë¦¬ í¬í•¨)
    2. HDFS -> ë²¡í„°í™” -> ì²˜ë¦¬ -> MongoDB
    """
    start_time = time.time()
    logger.info(f"ğŸš€ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    
    try:
        # 1. CSV -> HDFS ì ì¬ (bidprcAmtì˜ ì²« ë²ˆì§¸ ê°’ì„ recentprcë¡œ ì €ì¥)
        logger.info("1ï¸âƒ£ CSV -> HDFS ì ì¬ ì‹œì‘")
        csv_success = process_csv_files()
        if not csv_success:
            logger.error("âŒ CSV -> HDFS ì ì¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
        
        # 2. HDFS ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬
        spark = create_spark_session("Daily Bid Data Processing")
        
        try:
            # 2.1 ë°ì´í„° ë¡œë“œ (ì´ë¯¸ company_full ì •ë³´ê°€ ë³‘í•©ëœ ë°ì´í„°)
            logger.info("2ï¸âƒ£ HDFS ë°ì´í„° ë¡œë“œ")
            df = load_todays_data(spark)
            
            # 2.2 ë²¡í„°í™” (ë¶ˆìš©ì–´ ì²˜ë¦¬ í¬í•¨)
            logger.info("3ï¸âƒ£ í…ìŠ¤íŠ¸ ë²¡í„°í™”")
            df_vector = vectorize_data(spark, df)
            
            # 2.3 ë°ì´í„° ì²˜ë¦¬ (ì—…ì¢… ì½”ë“œë³„ ë¶„ë¥˜ ë° ID ë¶€ì—¬)
            logger.info("4ï¸âƒ£ ë°ì´í„° ì²˜ë¦¬ ë° ì»¬ë ‰ì…˜ ê²°ì •")
            no_limit_df, etc_df, industry_dfs = process_data(spark, df_vector)
            
            # 2.4 MongoDB ì €ì¥
            logger.info("5ï¸âƒ£ MongoDB ì €ì¥")
            save_to_mongodb(no_limit_df, etc_df, industry_dfs)
            
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            
        finally:
            # Spark ì„¸ì…˜ ì¢…ë£Œ
            if spark:
                spark.stop()
    
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    process_all()

if __name__ == "__main__":
    main()
