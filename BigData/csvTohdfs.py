# ~/Project/daily/csvTohdfs.py
"""
CSV -> HDFS ì ì¬ ëª¨ë“ˆ
CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  company_full ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ HDFSì— ì €ì¥
"""
import logging
import os
import subprocess
from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_extract, col, lit, udf, size, explode, struct, collect_list
from pyspark.sql.types import ArrayType, StructType, StructField, StringType
from pymongo import MongoClient
import config_daily as config
import pandas as pd
from hdfs import InsecureClient
from urllib.parse import urlparse

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# bizs_parsed ìŠ¤í‚¤ë§ˆ ì •ì˜
bizs_parsed_schema = ArrayType(StructType([
    StructField("opengRank", StringType()),
    StructField("prcbdrBizno", StringType()),
    StructField("prcbdrNm", StringType()),
    StructField("bidprcAmt", StringType()),
    StructField("bidprcrt", StringType()),
    StructField("rmrk", StringType()),
    StructField("emplyeNum", StringType()),
    StructField("indstrytyCd", StringType()),
    StructField("recentprc", StringType())
]))

@udf(bizs_parsed_schema)
def parse_bizs_column(bizs_series):
    """
    bizs ì»¬ëŸ¼ì„ íŒŒì‹±í•˜ëŠ” UDF í•¨ìˆ˜
    Row(...) êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ê³  ë”°ì˜´í‘œ ë‚´ ê´„í˜¸ë‚˜ íŠ¹ìˆ˜ë¬¸ìë„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬
    """
    if not bizs_series:
        return []
    
    result = []
    
    try:
        bizs_series = bizs_series.strip()
        
        # Row ì°¾ê¸° - ì¤‘ì²©ëœ ê´„í˜¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°œì„ 
        row_patterns = []
        paren_depth = 0
        quote_mode = False
        quote_char = None
        start_idx = -1
        
        # ì²« ë‹¨ê³„: Row ì‹œì‘ê³¼ ë ìœ„ì¹˜ ì°¾ê¸°
        for i, char in enumerate(bizs_series):
            # ë”°ì˜´í‘œ ëª¨ë“œ ì²˜ë¦¬
            if char in ["'", '"'] and (i == 0 or bizs_series[i-1] != '\\'):
                if not quote_mode:
                    quote_mode = True
                    quote_char = char
                elif char == quote_char:
                    quote_mode = False
                    quote_char = None
            
            # Row ì‹œì‘ ê°ì§€
            if not quote_mode and bizs_series[i:i+4] == 'Row(' and (i == 0 or bizs_series[i-1] not in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'):
                start_idx = i
                paren_depth = 0
            
            # ê´„í˜¸ ê¹Šì´ ì¶”ì 
            if not quote_mode:
                if char == '(' and start_idx != -1:
                    paren_depth += 1
                elif char == ')' and start_idx != -1:
                    paren_depth -= 1
                    if paren_depth == 0:  # Row(...) ì™„ë£Œ
                        row_patterns.append((start_idx, i))
                        start_idx = -1
        
        # ë‘ ë²ˆì§¸ ë‹¨ê³„: ê° Row íŒ¨í„´ íŒŒì‹±
        for start_idx, end_idx in row_patterns:
            row_content = bizs_series[start_idx+4:end_idx]  # 'Row(' ì œì™¸í•˜ê³  ì‹œì‘
            
            # í•„ë“œ íŒŒì‹±ì„ ìœ„í•œ ìƒíƒœ ë³€ìˆ˜
            fields = {}
            field_name = ""
            field_value = ""
            parsing_name = True
            in_quotes = False
            quote_char = None
            escaped = False
            
            i = 0
            while i < len(row_content):
                char = row_content[i]
                
                # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬
                if escaped:
                    field_value += char
                    escaped = False
                    i += 1
                    continue
                
                if char == '\\':
                    escaped = True
                    i += 1
                    continue
                
                # ë”°ì˜´í‘œ ëª¨ë“œ ì „í™˜
                if char in ["'", '"'] and not in_quotes:
                    in_quotes = True
                    quote_char = char
                    i += 1
                    continue
                elif char == quote_char and in_quotes:
                    in_quotes = False
                    i += 1
                    continue
                
                # í•„ë“œ ì´ë¦„-ê°’ êµ¬ë¶„
                if char == '=' and parsing_name and not in_quotes:
                    field_name = field_name.strip()
                    parsing_name = False
                    i += 1
                    continue
                
                # í•„ë“œ ì¢…ë£Œ (ì½¤ë§ˆ)
                if char == ',' and not in_quotes and not parsing_name:
                    fields[field_name] = field_value.strip()
                    field_name = ""
                    field_value = ""
                    parsing_name = True
                    i += 1
                    continue
                
                # ì¼ë°˜ ë¬¸ì ì¶”ê°€
                if parsing_name:
                    field_name += char
                else:
                    field_value += char
                
                i += 1
            
            # ë§ˆì§€ë§‰ í•„ë“œ ì²˜ë¦¬
            if field_name and not parsing_name:
                fields[field_name] = field_value.strip()
            
            # ê°’ ì •ë¦¬ - ë”°ì˜´í‘œ ì œê±°
            for key in fields:
                value = fields[key]
                if (value.startswith("'") and value.endswith("'")) or (value.startswith('"') and value.endswith('"')):
                    fields[key] = value[1:-1]
            
            # Row ë°ì´í„° ì¶”ê°€
            result.append({
                "opengRank": fields.get("opengRank", ""),
                "prcbdrBizno": fields.get("prcbdrBizno", ""),
                "prcbdrNm": fields.get("prcbdrNm", ""),
                "bidprcAmt": fields.get("bidprcAmt", ""),
                "bidprcrt": fields.get("bidprcrt", ""),
                "rmrk": fields.get("rmrk", ""),
                "emplyeNum": "",  # ê¸°ë³¸ê°’
                "indstrytyCd": "",  # ê¸°ë³¸ê°’
                "recentprc": ""  # ê¸°ë³¸ê°’ - ë‚˜ì¤‘ì— bidprcAmtì˜ ì²« ë²ˆì§¸ ê°’ìœ¼ë¡œ ëŒ€ì²´
            })
    except Exception as e:
        logger.error(f"âŒ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    
    return result

def create_spark_session():
    spark = SparkSession.builder \
        .appName("CSV to HDFS") \
        .master("local[4]") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:8020")\
        .config("spark.sql.shuffle.partitions", "64") \
        .config("spark.network.timeout", "1200s") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    return spark

def get_hdfs_client():
    return InsecureClient(config.get_hdfs_web_url())

# def check_hdfs_path(path, spark):
#     logger.info(f"ğŸ” HDFS ê²½ë¡œ í™•ì¸ ì¤‘ (Spark ë°©ì‹): {path}")
#     try:
#         fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
#         exists = fs.exists(spark._jvm.org.apache.hadoop.fs.Path(path))
#         logger.info(f"  - ê²°ê³¼: {'ìˆìŒ' if exists else 'ì—†ìŒ'}")
#         return exists
#     except Exception as e:
#         logger.error(f"âŒ Spark ê¸°ë°˜ ê²½ë¡œ ì²´í¬ ì‹¤íŒ¨: {e}")
#         return False
def check_hdfs_path(path):
    #ì „ë‹¬ : "hdfs://namenode:8020/user/hadoop/data/bids/20250408"
    logger.info(f"ğŸ” HDFS ê²½ë¡œ í™•ì¸ ì¤‘ (WebHDFS ë°©ì‹): {path}")
    try:
        parsed_path = urlparse(path).path  # ê²°ê³¼: /user/hadoop/data/bids/20250408
        client = get_hdfs_client() #http://namenode:9870 ì—ì„œ í™•ì¸.
        exists = client.status(parsed_path, strict=False) is not None
        #exists = client.status(path, strict=False) is not None
        logger.info(f"  - ê²°ê³¼: {'ìˆìŒ' if exists else 'ì—†ìŒ'}")
        return exists
    except Exception as e:
        logger.error(f"âŒ WebHDFS ê¸°ë°˜ ê²½ë¡œ ì²´í¬ ì‹¤íŒ¨: {e}")
        return False



def load_company_full_from_mongodb(spark):
    """MongoDBì—ì„œ company_full ë°ì´í„° ë¡œë“œ (recentprc í¬í•¨)"""
    try:
        logger.info("ğŸ“¦ MongoDBì—ì„œ company_full ë°ì´í„° ë¡œë“œ ì¤‘...")

        client = MongoClient(config.MONGO_URI)
        db = client[config.MONGO_DB_NAME]
        
        if "company_full" not in db.list_collection_names():
            raise ValueError("âŒ 'company_full' ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # MongoDB â†’ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œ
        company_list = list(db["company_full"].find())
        if not company_list:
            raise ValueError("âŒ company_full ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        client.close()

        # â— _id ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        for doc in company_list:
            if "_id" in doc:
                doc["_id"] = str(doc["_id"])

        # Pandas â†’ Spark ë³€í™˜
        pandas_df = pd.DataFrame(company_list)
        company_df = spark.createDataFrame(pandas_df)

        logger.info(f"âœ… MongoDB â†’ Spark DataFrame ë¡œë”© ì™„ë£Œ: {company_df.count()} rows")

        # recentprc ì¶”ì¶œìš© UDF ë“±ë¡
        @udf(StringType())
        def extract_first_bidprc(bidprc_value):
            if bidprc_value is None:
                return ""
            try:
                if isinstance(bidprc_value, str) and bidprc_value.startswith("[") and bidprc_value.endswith("]"):
                    items = bidprc_value.strip("[]").replace("'", "").replace('"', "").split(", ")
                    return items[0].strip() if items and items[0] else ""
                elif isinstance(bidprc_value, list) and len(bidprc_value) > 0:
                    return str(bidprc_value[0])
                return ""
            except Exception as e:
                logger.error(f"âŒ bidprcAmt íŒŒì‹± ì˜¤ë¥˜: {e} - ê°’: {bidprc_value}")
                return ""

        # recentprc ì»¬ëŸ¼ ì¶”ê°€
        if "bidprcAmt" in company_df.columns:
            company_df = company_df.withColumn("recentprc", extract_first_bidprc(col("bidprcAmt")))
            sample_result = company_df.filter(col("bidprcAmt").isNotNull()).limit(5)
            logger.info("ğŸ” recentprc ì¶”ì¶œ ìƒ˜í”Œ:")
            sample_result.select("prcbdrBizno", "bidprcAmt", "recentprc").show(truncate=False)
        else:
            logger.warning("âš ï¸ bidprcAmt ì»¬ëŸ¼ì´ ì—†ì–´ recentprcëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.")
            company_df = company_df.withColumn("recentprc", lit(""))

        return company_df

    except Exception as e:
        logger.error(f"âŒ company_full ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def enrich_bizs_parsed_with_company_info(spark, df, company_df):
    """bizs_parsedì— ê¸°ì—… ì •ë³´ ë³‘í•©"""
    logger.info("ğŸ”„ bizs_parsedì— company_full ë°ì´í„°ì—ì„œ ê¸°ì—… ì •ë³´ ë³‘í•© ì¤‘...")
    
    # company_dfì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    company_info = company_df.select(
        col("prcbdrBizno"),
        col("emplyeNum"),
        col("indstrytyCd"),
        col("recentprc")  # ì´ë¯¸ extract_first_bidprc UDFë¡œ ì²˜ë¦¬ëœ recentprc ì»¬ëŸ¼ (ì²« ë²ˆì§¸ ê°’)
    )
    
    # ì„ íƒëœ ì»¬ëŸ¼ í™•ì¸
    logger.info("ğŸ“‹ company_infoì—ì„œ ì„ íƒëœ ì»¬ëŸ¼:")
    for field in company_info.schema.fields:
        logger.info(f"  - {field.name}: {field.dataType}")
    
    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    sample_companies = company_info.limit(3)
    logger.info("\nğŸ” company_info ìƒ˜í”Œ ë°ì´í„°:")
    sample_companies.show(truncate=False)
    
    # ë°ì´í„°í”„ë ˆì„ì„ í…Œì´ë¸”ë¡œ ë“±ë¡
    df.createOrReplaceTempView("bids")
    company_info.createOrReplaceTempView("companies")
    
    # SQLì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì—… ì •ë³´ë¥¼ bizs_parsedì— ë³‘í•©
    enriched_df = spark.sql("""
    WITH exploded_bizs AS (
        SELECT 
            a.bidNtceNo,
            a.process_date,
            posexplode(a.bizs_parsed) AS (pos, biz)
        FROM bids a
    ),
    enriched_bizs AS (
        SELECT 
            b.bidNtceNo,
            b.process_date,
            b.pos,
            struct(
                b.biz.opengRank AS opengRank,
                b.biz.prcbdrBizno AS prcbdrBizno,
                b.biz.prcbdrNm AS prcbdrNm,
                b.biz.bidprcAmt AS bidprcAmt,
                b.biz.bidprcrt AS bidprcrt,
                b.biz.rmrk AS rmrk,
                COALESCE(c.emplyeNum, '') AS emplyeNum,
                COALESCE(c.indstrytyCd, '') AS indstrytyCd,
                COALESCE(c.recentprc, '') AS recentprc  /* ê¸°ì—… ì •ë³´ì—ì„œ ì¶”ì¶œí•œ ì²« ë²ˆì§¸ bidprcAmt ê°’ */
            ) AS enriched_biz
        FROM exploded_bizs b
        LEFT JOIN companies c ON b.biz.prcbdrBizno = c.prcbdrBizno
    ),
    grouped_bizs AS (
        SELECT 
            bidNtceNo,
            process_date,
            collect_list(enriched_biz) AS bizs_parsed_enriched
        FROM enriched_bizs
        GROUP BY bidNtceNo, process_date
    )
    SELECT 
        a.*,
        COALESCE(b.bizs_parsed_enriched, a.bizs_parsed) AS bizs_parsed_enriched
    FROM bids a
    LEFT JOIN grouped_bizs b ON a.bidNtceNo = b.bidNtceNo
    """)
    
    # ê¸°ì¡´ bizs_parsed ì»¬ëŸ¼ì„ ì œê±°í•˜ê³  ìƒˆë¡œìš´ bizs_parsed_enrichedë¡œ ëŒ€ì²´
    enriched_df = enriched_df.drop("bizs_parsed").withColumnRenamed("bizs_parsed_enriched", "bizs_parsed")
    
    # ìƒ˜í”Œ ì¶œë ¥ - ê¸°ì—… ì •ë³´ê°€ ë³‘í•©ëœ ê²°ê³¼ í™•ì¸
    sample_rows = enriched_df.select("bidNtceNo", "process_date", "bizs_parsed").limit(2).collect()
    logger.info("\nğŸ” ê¸°ì—… ì •ë³´ê°€ ë³‘í•©ëœ bizs_parsed ìƒ˜í”Œ:")
    
    for i, row in enumerate(sample_rows):
        logger.info(f"\nìƒ˜í”Œ {i+1}:")
        logger.info(f"  - bidNtceNo: {row['bidNtceNo']}")
        logger.info(f"  - process_date: {row['process_date']}")
        
        for j, biz in enumerate(row['bizs_parsed'][:2]):  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
            logger.info(f"    ê¸°ì—… {j+1}: {{opengRank: '{biz['opengRank']}', " +
                       f"prcbdrBizno: '{biz['prcbdrBizno']}', " +
                       f"emplyeNum: '{biz['emplyeNum']}', " +
                       f"indstrytyCd: '{biz['indstrytyCd']}', " +
                       f"recentprc: '{biz['recentprc']}'}}")  # recentprcëŠ” bidprcAmtì˜ ì²« ë²ˆì§¸ ê°’
    
    return enriched_df


def process_csv_files():
    """CSV íŒŒì¼ ì²˜ë¦¬ ë° HDFS ì ì¬"""
    spark = create_spark_session()
    output_path = config.get_today_data_path()
    #"hdfs://namenode:8020/user/hadoop/data/bids/20250408"

    try:
        # HDFS ê²½ë¡œ í™•ì¸ (ì‚­ì œëŠ” í•˜ì§€ ì•ŠìŒ)
        #path_exists = check_hdfs_path(output_path)
        path_exists = check_hdfs_path(output_path)
        if path_exists:
            logger.info(f"âš ï¸ ì˜¤ëŠ˜ ë‚ ì§œ ê²½ë¡œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}")
            logger.info("  - ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # company_full ë°ì´í„° ë¡œë“œ (bidprcAmt ì²« ë²ˆì§¸ ê°’ì„ recentprcë¡œ ì¶”ì¶œ)
        company_df = load_company_full_from_mongodb(spark)
        
        # ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
        total_bid_df = None
        
        # ê° CSV íŒŒì¼ ì²˜ë¦¬
        for file in config.CSV_FILES:
            file_path = os.path.join(config.CSV_BASE_PATH, f"{file}.csv")
            
            if os.path.exists(file_path):
                # íŒŒì¼ ë¡œë“œ
                logger.info(f"ğŸ“„ CSV íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
                #df = spark.read.option("header", True).csv(file_path)
                # ğŸš¨ Sparkì—ê²Œ ë¡œì»¬ íŒŒì¼ì„ì„ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì¤Œ
                spark_file_path = f"file://{file_path}"  
                df = spark.read.option("header", True).csv(spark_file_path)

                # lcnsLmtNm íŒŒì‹± ë° ì¶”ê°€ ì²˜ë¦¬
                df = df.withColumn("industry_code", regexp_extract(col("lcnsLmtNm"), r"/([0-9]+)$", 1)) \
                       .withColumn("lcnsLmtNm", regexp_extract(col("lcnsLmtNm"), r"^([^/]+)", 1))
                
                # bizs ì»¬ëŸ¼ íŒŒì‹±
                df = df.withColumn("bizs_parsed", parse_bizs_column(col("bizs")))
                
                # ë°ì´í„°í”„ë ˆì„ ë³‘í•©
                if total_bid_df is None:
                    total_bid_df = df
                else:
                    total_bid_df = total_bid_df.unionByName(df, allowMissingColumns=True)
                
                logger.info(f"  - {file} ì²˜ë¦¬ ì™„ë£Œ: {df.count()} í–‰")
            else:
                logger.warning(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        if total_bid_df is None:
            logger.error("âŒ ì²˜ë¦¬í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì²˜ë¦¬ ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
        total_bid_df = total_bid_df.withColumn("process_date", lit(config.TODAY))
        
        # bizs_parsed í†µê³„
        total_count = total_bid_df.count()
        non_empty_count = total_bid_df.filter(size(col("bizs_parsed")) > 0).count()
        empty_count = total_count - non_empty_count
        logger.info(f"\nğŸ“Š bizs_parsed í†µê³„:")
        logger.info(f"  - ì´ í–‰ ìˆ˜: {total_count}")
        logger.info(f"  - íŒŒì‹± ì„±ê³µ: {non_empty_count} í–‰ ({non_empty_count / total_count * 100:.2f}%)")
        logger.info(f"  - íŒŒì‹± ì‹¤íŒ¨: {empty_count} í–‰")
        
        # company_full ë°ì´í„°ë¡œ bizs_parsed ë³´ê°• (ì²« ë²ˆì§¸ bidprcAmtë¥¼ recentprcë¡œ)
        enriched_df = enrich_bizs_parsed_with_company_info(spark, total_bid_df, company_df)
        
        # HDFSì— ì €ì¥
        logger.info(f"ğŸ’¾ ë°ì´í„°ë¥¼ HDFSì— ì €ì¥ ì¤‘: {output_path}")
        # bizs ì›ë³¸ ì»¬ëŸ¼ì€ ì œì™¸í•˜ê³  ì €ì¥
        columns_to_drop = ["bizs"]
        enriched_df = enriched_df.drop(*columns_to_drop)
        enriched_df.write.mode("overwrite").parquet(output_path)
        logger.info(f"âœ… HDFS ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # ì €ì¥ ê²€ì¦
        logger.info("\nğŸ” HDFS ì €ì¥ ê²€ì¦")
        reloaded = spark.read.parquet(output_path)
        logger.info(f"  - ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜: {reloaded.count()}")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        sample = reloaded.select(
            "bidNtceNo", 
            explode("bizs_parsed").alias("biz")
        ).filter(col("biz.recentprc") != "").limit(5)
        
        logger.info("  - recentprc ìƒ˜í”Œ í™•ì¸:")
        sample.select("bidNtceNo", "biz.prcbdrBizno", "biz.recentprc").show(truncate=False)
        
        return True
    
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        if spark:
            spark.stop()

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª…ë ¹ì¤„ì—ì„œ ì§ì ‘ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger.info(f"ğŸš€ CSV -> HDFS ì ì¬ ì‹œì‘ ({config.TODAY})")
    success = process_csv_files()
    if success:
        logger.info("âœ… CSV -> HDFS ì ì¬ ì™„ë£Œ")
    else:
        logger.error("âŒ CSV -> HDFS ì ì¬ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
